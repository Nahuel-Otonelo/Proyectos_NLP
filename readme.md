# üì∞ Proyecto 1: Clasificaci√≥n de Texto con Naive Bayes (Dataset 20 Newsgroups)

Este proyecto es el primer desaf√≠o de la mater√≠a Procesamiento de Lenguaje Natural (PLN).

El notebook `Desafio_1.ipynb` explora la vectorizaci√≥n de texto (TF-IDF), la similaridad de documentos, la implementaci√≥n de clasificadores (k-NN y Naive Bayes) y la optimizaci√≥n de hiperpar√°metros.

## üìä Dataset

Se utiliz√≥ el dataset **20 Newsgroups**, cargado directamente desde `scikit-learn`. Este es un conjunto cl√°sico para la clasificaci√≥n de texto, compuesto por ~18,000 mensajes de foros distribuidos en 20 categor√≠as tem√°ticas (ej. `rec.autos`, `sci.med`, `talk.politics.misc`).

---

## üõ†Ô∏è Desaf√≠os y Metodolog√≠a

El notebook est√° dividido en los 4 puntos de la consigna:

### 1. An√°lisis de Similaridad de Documentos

Se vectoriz√≥ el corpus de `train` con `TfidfVectorizer`. Luego, se midi√≥ la similaridad coseno entre 5 documentos elegidos al azar y el resto del corpus para analizar la coherencia de las clases de los documentos m√°s similares.

### 2. Clasificador por Prototipos (k-NN)

Se construy√≥ un clasificador 1-NN ("prototipo") asignando la clase del vecino m√°s cercano. Como extensi√≥n, se implement√≥ un clasificador **k-NN** completo, probando un rango de $k$ (de 1 a 21) y comparando dos estrategias de votaci√≥n:

* **Voto Democr√°tico (`weights='uniform'`)**
* **Voto Calificado (`weights='distance'`)**

Se gener√≥ un gr√°fico para comparar el F1-Score de ambas estrategias y encontrar el $k$ √≥ptimo.

### 3. Optimizaci√≥n de Naive Bayes (GridSearch)

El objetivo era maximizar el `f1-score (macro)`:

1.  Se compar√≥ `MultinomialNB` vs. `ComplementNB`, identificando a `ComplementNB` como el modelo superior (probablemente por el desbalance de clases del dataset).
2.  Se implement√≥ un **`Pipeline`** de `scikit-learn` para encadenar el `TfidfVectorizer` y el `ComplementNB`.
3.  Se utiliz√≥ **`GridSearchCV`** para encontrar la mejor combinaci√≥n de hiperpar√°metros, previniendo el *data leakage* mediante validaci√≥n cruzada.

### 4. Similaridad de Palabras (Matriz Transpuesta)

Finalmente, se transpuso la matriz TF-IDF (documento-t√©rmino) para obtener una matriz (t√©rmino-documento).

* Cada fila se reinterpret√≥ como un **vector de palabra** (un embedding simple).
* Se analiz√≥ la similaridad coseno de 5 palabras (`god`, `car`, `president`, etc.) para estudiar las relaciones sem√°nticas y de co-ocurrencia que el modelo fue capaz de capturar.

---

# üì∞ Proyecto 1: Clasificaci√≥n de Texto con Naive Bayes (Dataset 20 Newsgroups)

Este proyecto es el primer desaf√≠o de la materia Procesamiento de Lenguaje Natural (PLN).

El notebook `Desafio_1.ipynb` explora la vectorizaci√≥n de texto (TF-IDF), la similaridad de documentos, la implementaci√≥n de clasificadores (k-NN y Naive Bayes) y la optimizaci√≥n de hiperpar√°metros.

## üìä Dataset

Se utiliz√≥ el dataset **20 Newsgroups**, cargado directamente desde `scikit-learn`. Este es un conjunto cl√°sico para la clasificaci√≥n de texto, compuesto por ~18,000 mensajes de foros distribuidos en 20 categor√≠as tem√°ticas (ej. `rec.autos`, `sci.med`, `talk.politics.misc`).

---


# üí¨ Desaf√≠o 2: Embeddings de Palabras con Word2Vec (Mart√≠n Fierro)

Este proyecto es el segundo desaf√≠o de la materia, enfocado en la creaci√≥n y an√°lisis de embeddings de palabras personalizados.

El notebook `Desafio_2.ipynb` implementa la librer√≠a `Gensim` para entrenar un modelo **Word2Vec (Skip-gram)**, utilizando un corpus de texto en espa√±ol.

## üìñ Dataset

Se utiliz√≥ un corpus personalizado compuesto por las dos obras de Jos√© Hern√°ndez: **"El Gaucho Mart√≠n Fierro"** y **"La Vuelta de Mart√≠n Fierro"**.

Los textos se obtuvieron del sitio `textos.info` en formato `.epub`, se convirtieron a `.txt` con una herramienta online y se limpiaron manualmente para retener √∫nicamente los versos del poema.

## üõ†Ô∏è Desaf√≠os y Metodolog√≠a

### 1. Preprocesamiento del Corpus (El Desaf√≠o Clave)

La estrategia para solucionar el inconveniente de versos cortos, fue tratar el poema como un solo bloque de **"prosa"** continuo:

* Se **reemplazaron todos los saltos de l√≠nea (`\n`) por espacios**, creando un √∫nico string de texto.
* Se utiliz√≥ **`text_to_word_sequence` de Keras/TensorFlow** para tokenizar. Esta funci√≥n se eligi√≥ porque:
    1.  Maneja correctamente caracteres especiales (como la `√º` en "vig√ºela"), a diferencia del filtro `.isalpha()` de NLTK. No reconoci√≥ vig√ºela pero si otras...
    2.  **No elimina las *stop words*** (como "el", "la", "que"), que son cruciales para que Word2Vec aprenda el contexto gramatical que nos faltaba. En una iteracion anterior de este trabajo, habia eliminado stop words y daba valores inconsistentes de similaridad.
* La lista gigante de tokens se dividi√≥ en **"fragmentos" (chunks)** de 100 palabras cada uno, que fueron los "documentos" que se pasaron a Gensim.

### 2. Entrenamiento de Word2Vec (Gensim)

Se ajustaron los hiperpar√°metros del modelo `Word2Vec` (Skip-gram) para un corpus denso pero peque√±o. Establecemos:

* `min_count=3`
* `window=5` (para capturar el contexto de la prosa)
* `vector_size=10

---

# ü§ñ Desaf√≠o 3: Modelos de Lenguaje a Nivel de Caracteres (RNN, LSTM, GRU)

Este proyecto es el tercer desaf√≠o de la materia, centrado en la generaci√≥n de texto utilizando modelos secuenciales profundos en PyTorch.

El notebook `desafio_3.ipynb` implementa y compara tres arquitecturas de redes neuronales recurrentes: **RNN**, **LSTM** y **GRU**, entrenadas para predecir el siguiente caracter en una secuencia.

## üìñ Dataset

Como corpus, se utilizo el  **"Robinson Crusoe"** de Daniel Defoe, descargado de textos.info.

## üõ†Ô∏è Desaf√≠os y Metodolog√≠a

### 1. Implementaci√≥n en PyTorch

Se definieron tres clases de modelos (`RNNModel`, `LSTMModel`, `GRUModel`), todas compartiendo una estructura similar pero variando en la capa recurrente:
*   **Embedding**: One-hot encoding de los caracteres.
*   **Capa Recurrente**: RNN, LSTM o GRU.
*   **Dropout**: Se incorpor√≥ una capa de `Dropout(0.1)` para regularizaci√≥n


### 2. Entrenamiento

Se cre√≥ una funci√≥n de entrenamiento reutilizable `train_and_evaluate` que incluye:
*   **Early Stopping**: Basado en la perplejidad del conjunto de validaci√≥n (paciencia de 5 epochs).
*   **Checkpointing**: Guardado autom√°tico del mejor modelo.
*   **Visualizaci√≥n**: Gr√°ficos de la evoluci√≥n de la perplejidad.

### 3. Generaci√≥n de Texto (Beam Search)

Se implement√≥ un algoritmo de **Stochastic Beam Search** para generar texto, permitiendo controlar la aleatoriedad mediante un par√°metro de **temperatura**.

## üìä Conclusiones y Resultados

### Entrenamiento
*   **RNN**: Mostr√≥ un aprendizaje m√°s lento, requiriendo m√°s √©pocas para converger.
*   **LSTM y GRU**: Aprendieron significativamente m√°s r√°pido, logrando mejores m√©tricas en menos epochs. Sin embargo, mostraron una tendencia mayor al sobreajuste, activando el *early stopping* antes que el modelo **RNN**.
*   **Regularizaci√≥n**: La inclusi√≥n de Dropout (0.1) fue clave para mejorar la generalizaci√≥n.

### Generaci√≥n
*   A pesar de trabajar car√°cter por car√°cter, los modelos aprendieron impl√≠citamente la morfolog√≠a del lenguaje, generando en su gran mayor√≠a palabras v√°lidas en lugar de secuencias aleatorias.
*   Los modelos LSTM y GRU produjeron textos sint√°cticamente m√°s coherentes que la RNN simple.
*   **Temperatura**:
    *   `0.1` (Baja): Texto coherente y algo conservador.
    *   `1.0` (Media): Buen balance entre coherencia y variedad.
    *   `2.0` (Alta): Resultados m√°s ca√≥ticos y "raros", como era de esperarse.