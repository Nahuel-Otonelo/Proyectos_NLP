# üì∞ Proyecto 1: Clasificaci√≥n de Texto con Naive Bayes (Dataset 20 Newsgroups)

Este proyecto es el primer desaf√≠o de la mater√≠a Procesamiento de Lenguaje Natural (PLN).

El notebook `Desafio_1.ipynb` explora la vectorizaci√≥n de texto (TF-IDF), la similaridad de documentos, la implementaci√≥n de clasificadores (k-NN y Naive Bayes) y la optimizaci√≥n de hiperpar√°metros.

## üìä Dataset

Se utiliz√≥ el dataset **20 Newsgroups**, cargado directamente desde `scikit-learn`. Este es un conjunto cl√°sico para la clasificaci√≥n de texto, compuesto por ~18,000 mensajes de foros distribuidos en 20 categor√≠as tem√°ticas (ej. `rec.autos`, `sci.med`, `talk.politics.misc`).

---

## üõ†Ô∏è Desaf√≠os y Metodolog√≠a

El notebook est√° dividido en los 4 puntos de la consigna:

### 1. An√°lisis de Similaridad de Documentos

Se vectoriz√≥ el corpus de `train` con `TfidfVectorizer`. Luego, se midi√≥ la similaridad coseno entre 5 documentos elegidos al azar y el resto del corpus para analizar la coherencia de las clases de los documentos m√°s similares.

### 2. Clasificador por Prototipos (k-NN)

Se construy√≥ un clasificador 1-NN ("prototipo") asignando la clase del vecino m√°s cercano. Como extensi√≥n, se implement√≥ un clasificador **k-NN** completo, probando un rango de $k$ (de 1 a 21) y comparando dos estrategias de votaci√≥n:

* **Voto Democr√°tico (`weights='uniform'`)**
* **Voto Calificado (`weights='distance'`)**

Se gener√≥ un gr√°fico para comparar el F1-Score de ambas estrategias y encontrar el $k$ √≥ptimo.

### 3. Optimizaci√≥n de Naive Bayes (GridSearch)

El objetivo era maximizar el `f1-score (macro)`:

1.  Se compar√≥ `MultinomialNB` vs. `ComplementNB`, identificando a `ComplementNB` como el modelo superior (probablemente por el desbalance de clases del dataset).
2.  Se implement√≥ un **`Pipeline`** de `scikit-learn` para encadenar el `TfidfVectorizer` y el `ComplementNB`.
3.  Se utiliz√≥ **`GridSearchCV`** para encontrar la mejor combinaci√≥n de hiperpar√°metros, previniendo el *data leakage* mediante validaci√≥n cruzada.

### 4. Similaridad de Palabras (Matriz Transpuesta)

Finalmente, se transpuso la matriz TF-IDF (documento-t√©rmino) para obtener una matriz (t√©rmino-documento).

* Cada fila se reinterpret√≥ como un **vector de palabra** (un embedding simple).
* Se analiz√≥ la similaridad coseno de 5 palabras (`god`, `car`, `president`, etc.) para estudiar las relaciones sem√°nticas y de co-ocurrencia que el modelo fue capaz de capturar.

---

# üì∞ Proyecto 1: Clasificaci√≥n de Texto con Naive Bayes (Dataset 20 Newsgroups)

Este proyecto es el primer desaf√≠o de la materia Procesamiento de Lenguaje Natural (PLN).

El notebook `Desafio_1.ipynb` explora la vectorizaci√≥n de texto (TF-IDF), la similaridad de documentos, la implementaci√≥n de clasificadores (k-NN y Naive Bayes) y la optimizaci√≥n de hiperpar√°metros.

## üìä Dataset

Se utiliz√≥ el dataset **20 Newsgroups**, cargado directamente desde `scikit-learn`. Este es un conjunto cl√°sico para la clasificaci√≥n de texto, compuesto por ~18,000 mensajes de foros distribuidos en 20 categor√≠as tem√°ticas (ej. `rec.autos`, `sci.med`, `talk.politics.misc`).

---


# üí¨ Desaf√≠o 2: Embeddings de Palabras con Word2Vec (Mart√≠n Fierro)

Este proyecto es el segundo desaf√≠o de la materia, enfocado en la creaci√≥n y an√°lisis de embeddings de palabras personalizados.

El notebook `Desafio_2.ipynb` implementa la librer√≠a `Gensim` para entrenar un modelo **Word2Vec (Skip-gram)**, utilizando un corpus de texto en espa√±ol.

## üìñ Dataset

Se utiliz√≥ un corpus personalizado compuesto por las dos obras de Jos√© Hern√°ndez: **"El Gaucho Mart√≠n Fierro"** y **"La Vuelta de Mart√≠n Fierro"**.

Los textos se obtuvieron del sitio `textos.info` en formato `.epub`, se convirtieron a `.txt` con una herramienta online y se limpiaron manualmente para retener √∫nicamente los versos del poema.

## üõ†Ô∏è Desaf√≠os y Metodolog√≠a

### 1. Preprocesamiento del Corpus (El Desaf√≠o Clave)

La estrategia para solucionar el inconveniente de versos cortos, fue tratar el poema como un solo bloque de **"prosa"** continuo:

* Se **reemplazaron todos los saltos de l√≠nea (`\n`) por espacios**, creando un √∫nico string de texto.
* Se utiliz√≥ **`text_to_word_sequence` de Keras/TensorFlow** para tokenizar. Esta funci√≥n se eligi√≥ porque:
    1.  Maneja correctamente caracteres especiales (como la `√º` en "vig√ºela"), a diferencia del filtro `.isalpha()` de NLTK. No reconoci√≥ vig√ºela pero si otras...
    2.  **No elimina las *stop words*** (como "el", "la", "que"), que son cruciales para que Word2Vec aprenda el contexto gramatical que nos faltaba. En una iteracion anterior de este trabajo, habia eliminado stop words y daba valores inconsistentes de similaridad.
* La lista gigante de tokens se dividi√≥ en **"fragmentos" (chunks)** de 100 palabras cada uno, que fueron los "documentos" que se pasaron a Gensim.

### 2. Entrenamiento de Word2Vec (Gensim)

Se ajustaron los hiperpar√°metros del modelo `Word2Vec` (Skip-gram) para un corpus denso pero peque√±o. Establecemos:

* `min_count=3`
* `window=5` (para capturar el contexto de la prosa)
* `vector_size=10