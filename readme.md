
# üì∞ Proyecto 1: Clasificaci√≥n de Texto con Naive Bayes (Dataset 20 Newsgroups)

Este proyecto es el primer desaf√≠o de la materia Procesamiento de Lenguaje Natural (PLN).

El notebook `Desafio_1.ipynb` explora la vectorizaci√≥n de texto (TF-IDF), la similaridad de documentos, la implementaci√≥n de clasificadores (k-NN y Naive Bayes) y la optimizaci√≥n de hiperpar√°metros.

## üìä Dataset

Se utiliz√≥ el dataset **20 Newsgroups**, cargado directamente desde `scikit-learn`. Este es un conjunto cl√°sico para la clasificaci√≥n de texto, compuesto por ~18,000 mensajes de foros distribuidos en 20 categor√≠as tem√°ticas (ej. `rec.autos`, `sci.med`, `talk.politics.misc`).

---


# üí¨ Desaf√≠o 2: Embeddings de Palabras con Word2Vec (Mart√≠n Fierro)

Este proyecto es el segundo desaf√≠o de la materia, enfocado en la creaci√≥n y an√°lisis de embeddings de palabras personalizados.

El notebook `Desafio_2.ipynb` implementa la librer√≠a `Gensim` para entrenar un modelo **Word2Vec (Skip-gram)**, utilizando un corpus de texto en espa√±ol.

## üìñ Dataset

Se utiliz√≥ un corpus personalizado compuesto por las dos obras de Jos√© Hern√°ndez: **"El Gaucho Mart√≠n Fierro"** y **"La Vuelta de Mart√≠n Fierro"**.

Los textos se obtuvieron del sitio `textos.info` en formato `.epub`, se convirtieron a `.txt` con una herramienta online y se limpiaron manualmente para retener √∫nicamente los versos del poema.

## üõ†Ô∏è Desaf√≠os y Metodolog√≠a

### 1. Preprocesamiento del Corpus (El Desaf√≠o Clave)

La estrategia para solucionar el inconveniente de versos cortos, fue tratar el poema como un solo bloque de **"prosa"** continuo:

* Se **reemplazaron todos los saltos de l√≠nea (`\n`) por espacios**, creando un √∫nico string de texto.
* Se utiliz√≥ **`text_to_word_sequence` de Keras/TensorFlow** para tokenizar. Esta funci√≥n se eligi√≥ porque:
    1.  Maneja correctamente caracteres especiales (como la `√º` en "vig√ºela"), a diferencia del filtro `.isalpha()` de NLTK. No reconoci√≥ vig√ºela pero si otras...
    2.  **No elimina las *stop words*** (como "el", "la", "que"), que son cruciales para que Word2Vec aprenda el contexto gramatical que nos faltaba. En una iteracion anterior de este trabajo, habia eliminado stop words y daba valores inconsistentes de similaridad.
* La lista gigante de tokens se dividi√≥ en **"fragmentos" (chunks)** de 100 palabras cada uno, que fueron los "documentos" que se pasaron a Gensim.

### 2. Entrenamiento de Word2Vec (Gensim)

Se ajustaron los hiperpar√°metros del modelo `Word2Vec` (Skip-gram) para un corpus denso pero peque√±o. Establecemos:

* `min_count=3`
* `window=5` (para capturar el contexto de la prosa)
* `vector_size=10

---

# ü§ñ Desaf√≠o 3: Modelos de Lenguaje a Nivel de Caracteres (RNN, LSTM, GRU)

Este proyecto es el tercer desaf√≠o de la materia, centrado en la generaci√≥n de texto utilizando modelos secuenciales profundos en PyTorch.

El notebook `desafio_3.ipynb` implementa y compara tres arquitecturas de redes neuronales recurrentes: **RNN**, **LSTM** y **GRU**, entrenadas para predecir el siguiente caracter en una secuencia.

## üìñ Dataset

Como corpus, se utilizo el  **"Robinson Crusoe"** de Daniel Defoe, descargado de textos.info.

## üõ†Ô∏è Desaf√≠os y Metodolog√≠a

### 1. Implementaci√≥n en PyTorch

Se definieron tres clases de modelos (`RNNModel`, `LSTMModel`, `GRUModel`), todas compartiendo una estructura similar pero variando en la capa recurrente:
*   **Embedding**: One-hot encoding de los caracteres.
*   **Capa Recurrente**: RNN, LSTM o GRU.
*   **Dropout**: Se incorpor√≥ una capa de `Dropout(0.1)` para regularizaci√≥n


### 2. Entrenamiento

Se cre√≥ una funci√≥n de entrenamiento reutilizable `train_and_evaluate` que incluye:
*   **Early Stopping**: Basado en la perplejidad del conjunto de validaci√≥n (paciencia de 5 epochs).
*   **Checkpointing**: Guardado autom√°tico del mejor modelo.
*   **Visualizaci√≥n**: Gr√°ficos de la evoluci√≥n de la perplejidad.

### 3. Generaci√≥n de Texto (Beam Search)

Se implement√≥ un algoritmo de **Stochastic Beam Search** para generar texto, permitiendo controlar la aleatoriedad mediante un par√°metro de **temperatura**.

--- 

# üåê Desaf√≠o 4: Traductor Ingl√©s-Espa√±ol con LSTM (Seq2Seq)

Este proyecto es el cuarto desaf√≠o de la materia, enfocado en la construcci√≥n de un modelo de traducci√≥n autom√°tica  utilizando una arquitectura **Encoder-Decoder**.

El notebook `desafio_4.ipynb` implementa un modelo **Seq2Seq con capas LSTM** en Keras/TensorFlow, optimizado para manejar un volumen considerable de datos sin saturar los recursos de memoria.

## üìñ Dataset

Se utiliz√≥ el dataset del **Tatoeba Project** (par ingl√©s-espa√±ol), que consiste en miles de oraciones traducidas.
Para este desaf√≠o, se logr√≥ escalar el entrenamiento a **25,000 pares de oraciones** (frente a las 6,000 originales), gracias a las optimizaciones de memoria implementadas.

## üõ†Ô∏è Desaf√≠os y Metodolog√≠a

### 1. Optimizaci√≥n de Memoria (El Cambio Cr√≠tico)

El principal obst√°culo t√©cnico fue el consumo de RAM al intentar escalar el dataset. El enfoque original utilizaba *Categorical Crossentropy*, lo que obligaba a convertir las secuencias de salida a matrices *One-Hot* gigantescas ($N_{samples} \times L_{sequence} \times V_{vocab}$).

**Soluci√≥n:** Se migr√≥ a **`sparse_categorical_crossentropy`**. Esto permiti√≥ trabajar directamente con los √≠ndices enteros de los tokens, reduciendo dr√°sticamente el uso de memoria y permitiendo cuadruplicar el tama√±o del dataset de entrenamiento.

### 2. Arquitectura del Modelo (Encoder-Decoder)

Se dise√±√≥ una arquitectura Seq2Seq cl√°sica pero robusta:
*   **Embeddings Pre-entrenados**: Se utilizaron vectores **GloVe** (Twitter 27B, 50d) para inicializar la capa de embedding del encoder, aprovechando conocimiento sem√°ntico previo.
*   **Encoder**: Una capa LSTM que procesa la secuencia de entrada y pasa sus estados internos ($h$, $c$) al decoder.
*   **Decoder**: Una capa LSTM que genera la traducci√≥n paso a paso, condicionada por los estados del encoder y la palabra generada anteriormente.
*   **Regularizaci√≥n**: Se incorpor√≥ **Dropout (0.2)** en las celdas LSTM para mitigar el sobreajuste, crucial dado que las oraciones son cortas y repetitivas.
### 3. Entrenamiento Inteligente
En lugar de un entrenamiento fijo, se implement√≥ una estrategia din√°mica:
*   **Early Stopping**: Monitoreo de la `val_loss` con paciencia de 3 √©pocas para detener el entrenamiento cuando el modelo deja de aprender.
*   **Model Checkpoint**: Guardado autom√°tico de los **mejores pesos** (`translator_model_best.weights.h5`), asegurando que el modelo final sea el √≥ptimo y no simplemente el √∫ltimo.

## üìä Resultados e Inferencia

Se construy√≥ una **infraestructura de inferencia separada** que reutiliza los pesos entrenados pero desacopla el encoder y el decoder. Esto permite realizar la traducci√≥n paso a paso (*step-by-step decoding*), inyectando la predicci√≥n actual como entrada para el siguiente paso temporal hasta encontrar el token de fin de oraci√≥n `<eos>`.

**üöÄ Modelo Pre-entrenado Disponible**
El repositorio incluye el archivo `translator_model_best.weights.h5` (~76MB) con los pesos del modelo ya entrenado.
*   **No es necesario re-entrenar:** El notebook detecta autom√°ticamente si este archivo existe. Si es as√≠, carga los pesos y salta directamente a la secci√≥n de inferencia, permitiendo probar las traducciones de inmediato.
*   **Resultados:** El modelo es capaz de generar traducciones coherentes para oraciones dentro del dominio del dataset de entrenamiento.